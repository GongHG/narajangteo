{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 나라장터 입찰공고 크롤링 with Python3</center>\n",
    "\n",
    "나라장터에 올라오는 입찰공고를 모니터링하기 위해 개발된 간단한 프로그램으로, 검색어 리스트를 설정하면 그에 따라 최근 7일간 공고된 입찰공고 리스트를 가져와 엑셀파일로 정리해줍니다. 크롤링 프로그램이지만, BeautifulSoup을 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import datetime, time\n",
    "import string\n",
    "from time import localtime, strftime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from xlsxwriter.utility import xl_col_to_name, xl_range\n",
    "from lxml import html\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category.txt 파일을 읽어오는 function을 만들어 리스트를 가져옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to read txt files and parse the list\n",
    "def txt_reader(name):\n",
    "    a=[]\n",
    "    f = open(name+\".txt\",'rb').readlines()\n",
    "    for item in f:\n",
    "        b = item.decode('utf-8').replace('\\n','')\n",
    "        a.append(b[:14])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the categories with the txt_reader function\n",
    "category_list = txt_reader('category')\n",
    "print(\"category.txt에 저장된 키워드를 가져옵니다: \" +str(category_list).replace('[','').replace(']','').replace(\"'\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목록 크롤링 function을 만듭니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreaPageScraper(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def request_url(self,cat):\n",
    "        '''각 키워드 별 url을 가져옵니다. returns url for a  category'''\n",
    "        #d는 오늘 날짜. end_date/toBidDt 값으로 사용합니다. \n",
    "        d = datetime.date.today()\n",
    "        end_date =str(d.strftime(\"%Y/%m/%d\"))\n",
    "        toBidDt = requests.utils.quote(end_date, safe='')\n",
    "        #fromtd로 일 주일 치 결과만을 가져옵니다. start_date/fromBidDt 값으로 사용합니다. \n",
    "        fromtd = d - timedelta(days=7)\n",
    "        start_date = str(fromtd.strftime(\"%Y/%m/%d\"))\n",
    "        fromBidDt = requests.utils.quote(start_date, safe='')\n",
    "        #키워드를 인코딩해 bidNm 값으로 사용합니다. \n",
    "        bidNm = requests.utils.quote(cat.encode('euc-kr'))\n",
    "        #FYI: 검색어 뿐만 아니라 추가적인 parameter를 여기에다가 추가할 수도 있습니다. \n",
    "        url = \"http://www.g2b.go.kr:8101/ep/tbid/tbidList.do?taskClCds=&bidNm=\" + bidNm + \"&searchDtType=1&fromBidDt=\" + fromBidDt + \"&toBidDt=\" + toBidDt + \"&fromOpenBidDt=&toOpenBidDt=&radOrgan=1&instNm=&exceptEnd=Y&area=&regYn=Y&bidSearchType=1&searchType=1&recordCountPerPage=1000\"\n",
    "        return url\n",
    "\n",
    "    def scrape_cat(self,cat):\n",
    "        '''키워드를 검색합니다. searches for each category'''\n",
    "        #위의 request_url function을 통해 생성된 url을 가봅니다. \n",
    "        cat_url = self.request_url(cat)\n",
    "        #pandas의 read_html 기능을 이용해 테이블을 가져옵니다. \n",
    "        df = pd.read_html(cat_url)[0]\n",
    "        #테이블에 'search_term'이라는 항목을 추가해 어떤 키워드로 검색해서 공고가 나왔는 지에 대한 정보를 추가합니다. \n",
    "        df['search_term']=cat\n",
    "        return df\n",
    "    \n",
    "    def get_bidurl(self,bidnum):\n",
    "        '''공고 상세페이지 url을 가져옵니다. gets the bid url based on the bid registration number \n",
    "        (ones that do not have a proper bid registration number usually doesn't have a corresponding link and would ask the user to go to the organization website for more informatioin)'''\n",
    "        num_split = str(bidnum).split(sep='-')\n",
    "        bidno = num_split[0]\n",
    "        if len(bidno) == 11:\n",
    "            bidseq = num_split[-1]\n",
    "            bidurl = \"http://www.g2b.go.kr:8081/ep/invitation/publish/bidInfoDtl.do?bidno=\"+bidno+\"&bidseq=\"+bidseq\n",
    "            return bidurl\n",
    "        else: \n",
    "            return \"Check organization website (공고기관) for details\"\n",
    "        bidseq = refnum_split[-1]\n",
    "        bidurl = \"http://www.g2b.go.kr:8081/ep/invitation/publish/bidInfoDtl.do?bidno=\"+bidno+\"&bidseq=\"+bidseq\n",
    "        return bidurl\n",
    "\n",
    "    def scrape_categories(self, categories):\n",
    "        '''각 키워드 별 리스트를 긁어옵니다. scrapes each keyword and compiles it into a list. \n",
    "        There is a 1 second delay between each search term to prevent getting blocked out of the site'''\n",
    "        appended_df = []\n",
    "        for category in tqdm(categories):\n",
    "            one_df = self.scrape_cat(category)\n",
    "            appended_df.append(one_df)\n",
    "            time.sleep(1)\n",
    "        appended_df = pd.concat(appended_df, axis = 0)\n",
    "        urlist=[]\n",
    "        for index,row in appended_df.iterrows():\n",
    "            urlist.append(self.get_bidurl(row['공고번호-차수']))\n",
    "            \n",
    "        appended_df['url']=urlist\n",
    "        return appended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape with the \"KoreaPageScraper\" class\n",
    "myscraper = KoreaPageScraper()\n",
    "\n",
    "df = myscraper.scrape_categories(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(df))+\"개의 공고를 찾았습니다. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제외할 키워드를 가져옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the excluding keywords\n",
    "excluding=txt_reader('exclude')\n",
    "print(\"exclude.txt에서 제외할 키워드를 가져옵니다: \"+str(excluding).replace('[','').replace(']','').replace(\"'\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_excluding = str(excluding).replace('[','').replace(']','').replace(\"'\",\"\").replace(\", \",\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the excluding keywords and informing how many lines were deleted. \n",
    "og = len(df)\n",
    "df = df[-df.공고명.str.contains(contains_excluding).fillna(True)]\n",
    "print(str(og-len(df))+\"개의 공고를 제외하였음. (현재 \"+str(len(df))+\"개의 공고가 남아있음)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 남아있는 공고 리스트를 클린업합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(df):\n",
    "    #Delete duplicates (more than two keywords together)\n",
    "    og2 = len(df)\n",
    "    df = df[~df.duplicated(['공고명'])].copy()\n",
    "    print(str(og2-len(df))+\"개의 중복 항목이 발견되어 삭제하였습니다. (현재 \"+str(len(df))+\"개의 공고가 남아있음)\")\n",
    "    #Divide the register date and due date\n",
    "    df['register_date'],df['duedate'] = df['입력일시(입찰마감일시)'].str.split('(', 1).str\n",
    "    df['duedate']=df['duedate'].str.replace(')','').replace('-','')\n",
    "    df = df.drop('입력일시(입찰마감일시)',axis=1)\n",
    "    #Sort the values by duedate. To sort with a different value, change the following line's 'duedate' with the column name you desire to sort it by. \n",
    "    column_sort = 'duedate'\n",
    "    df = df.sort_values(by=column_sort,ascending=False)\n",
    "    print(\"현재 공고 나열 순서는 '\"+column_sort+\"' 항목 내림차순입니다. 이를 바꾸기 위해서는 툴 관리자에게 연락 바랍니다. \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the df to make more sense\n",
    "clean_df = clean_up(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가격 정보를 추가합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the price information\n",
    "class AdditionalInfo(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_tree(self,page):\n",
    "        r=requests.get(page)\n",
    "        tree = html.fromstring(r.content)\n",
    "        return tree\n",
    "\n",
    "    def ext_link(self,page):\n",
    "        tree = self.get_tree(page)\n",
    "        file_link = tree.xpath('//*[@id=\"container\"]/div[17]/table/tbody/tr[*]/td[3]/div/a')\n",
    "        linklist = []\n",
    "        for links in file_link:\n",
    "            a = links.values()[0]\n",
    "            b = a[a.find(\"(\")+1:].split(',')[0].replace(\"'\",'')\n",
    "            c = \"http://www.g2b.go.kr:8081/ep/co/fileDownload.do?fileTask=NOTIFY&fileSeq=\"+b\n",
    "            linklist.append(c)\n",
    "        return linklist\n",
    "\n",
    "    def price(self,page):\n",
    "        tree = self.get_tree(page)\n",
    "        table_ptag = None\n",
    "        for l in tree.xpath('//*[@class=\"section\"]/p'):\n",
    "            if l.text.startswith('예정가격'):\n",
    "                table_ptag = l\n",
    "                break\n",
    "\n",
    "        x=table_ptag.getparent()\n",
    "        budget_table = pd.read_html(html.tostring(x))[0]\n",
    "        baejung = budget_table[1][1]\n",
    "        baejung = int(baejung[:baejung.find('원')].replace(\",\",\"\").replace(\"￦\",\"\"))\n",
    "        return baejung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AdditionalInfo()\n",
    "print(\"가격 정보를 가져옵니다:\")\n",
    "test_list = []\n",
    "for index,row in tqdm(clean_df.iterrows(), total=len(clean_df)):\n",
    "    try:\n",
    "        p = x.price(row.url)\n",
    "    except:\n",
    "        p = None\n",
    "    test_list.append(p)\n",
    "\n",
    "clean_df['budget'] = test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엑셀 파일에 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_excel(object):\n",
    "    def get_length(self,column):\n",
    "        valueex = column[~column.isnull()].reset_index(drop=True)[0]\n",
    "        if type(valueex) == str:\n",
    "            len_list = list(column.dropna().apply(lambda x: len(str(x))))\n",
    "            maxlen = max(len_list)\n",
    "            medlen = np.median(len_list)\n",
    "            meanlen = np.mean(len_list)\n",
    "            diff = maxlen-medlen\n",
    "            stdlen = np.std(len_list)\n",
    "            #min(A,B+C*numchars)\n",
    "            if maxlen < 10:\n",
    "                return maxlen+5\n",
    "            elif diff > 50:\n",
    "                if medlen == 0:\n",
    "                    return min(55,meanlen+5)\n",
    "                return medlen\n",
    "            elif maxlen < 50:\n",
    "                return meanlen+15\n",
    "            else:\n",
    "                return 50\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    def to_excel(self,df,name):\n",
    "        #Next step, format the excel file\n",
    "        print(\"saving the \"+name+\" list...\")\n",
    "        docname = \"나라장터_입찰공고-\"+name+\"-\"+str(strftime(\"%y%m%d(%H%M%S)\", localtime()))+\".xlsx\"\n",
    "        #make the destination directory, but guard against race condition\n",
    "        if not os.path.exists(name):\n",
    "            try:\n",
    "                os.makedirs(name)\n",
    "            except OSError as exc: \n",
    "                print(exc)\n",
    "                raise Exception('something failed')\n",
    "        writer = pd.ExcelWriter(\"%s/%s\"%(name,docname), engine='xlsxwriter')\n",
    "        df.to_excel(writer,index=False,sheet_name='Sheet1')\n",
    "        workbook  = writer.book\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        tablerange = xl_range(0,0,len(df),len(df.columns)-1)\n",
    "        headerrange = xl_range(0,0,0,len(df.columns)-1)\n",
    "        contentrange = xl_range(1,0,len(df),len(df.columns)-1)\n",
    "\n",
    "        #Formatting headers\n",
    "        header_format = workbook.add_format({'bg_color':'black'})\n",
    "        column_format = workbook.add_format({'bottom':True,'bg_color':'white'})\n",
    "        link_format = workbook.add_format({'font_color':'#157993','underline':True})\n",
    "        \n",
    "        # Set the column width and format.\n",
    "        columns = []\n",
    "        widths = []\n",
    "        for i in range(0,len(df.columns)):\n",
    "            a = xl_col_to_name(i)+\":\"+xl_col_to_name(i)\n",
    "            columns.append(a)\n",
    "            widths.append(self.get_length(df[df.columns[i]])) \n",
    "        \n",
    "        for c,w in zip(columns,widths):\n",
    "            worksheet.set_column(c, w)\n",
    "        \n",
    "        worksheet.conditional_format(contentrange,{'type':'no_errors',\n",
    "                                                   'format':column_format})\n",
    "        worksheet.conditional_format(headerrange,{'type':'no_errors',\n",
    "                                                  'format':header_format})\n",
    "        worksheet.conditional_format(tablerange,{'type':'text',\n",
    "                                                 'criteria':'containing',\n",
    "                                                 'value':'Click link',\n",
    "                                                 'format':link_format})\n",
    "           \n",
    "        #Formatting for putting in the header titles\n",
    "        table_headers = [{'header':c} for c in  df.columns]\n",
    "        #Create a table with the data\n",
    "        worksheet.add_table(tablerange,{'columns' : table_headers})         \n",
    "        \n",
    "        writer.save()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_to_excel = create_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_to_excel.to_excel(clean_df,'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('All done! Please hit Enter to exit this command prompt. ')\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call(['explorer','full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
