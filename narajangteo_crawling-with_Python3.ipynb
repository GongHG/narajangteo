{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 나라장터 입찰공고 크롤링 with Python3</center>\n",
    "\n",
    "나라장터에 올라오는 입찰공고를 모니터링하기 위해 개발된 간단한 프로그램으로, 검색어 리스트를 설정하면 그에 따라 최근 7일간 공고된 입찰공고 리스트를 가져와 엑셀파일로 정리해줍니다. 크롤링 프로그램이지만, BeautifulSoup을 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import datetime, time\n",
    "import string\n",
    "from time import localtime, strftime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from xlsxwriter.utility import xl_col_to_name, xl_range\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KoreaPageScraper(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def request_url(self,cat):\n",
    "        '''returns url for a  category'''\n",
    "        d = datetime.date.today()\n",
    "        fromtd = d - timedelta(days=7)\n",
    "        start_date = str(fromtd.strftime(\"%Y/%m/%d\"))\n",
    "        end_date =str(d.strftime(\"%Y/%m/%d\"))\n",
    "        fromBidDt = requests.utils.quote(start_date, safe='')\n",
    "        toBidDt = requests.utils.quote(end_date, safe='')\n",
    "        bidNm = requests.utils.quote(cat.encode('euc-kr'))\n",
    "        url = \"http://www.g2b.go.kr:8101/ep/tbid/tbidList.do?taskClCds=&bidNm=\" + bidNm + \"&searchDtType=1&fromBidDt=\" + fromBidDt + \"&toBidDt=\" + toBidDt + \"&fromOpenBidDt=&toOpenBidDt=&radOrgan=1&instNm=&exceptEnd=Y&area=&regYn=Y&bidSearchType=1&searchType=1&recordCountPerPage=1000\"\n",
    "        return url\n",
    "\n",
    "    def scrape_cat(self,cat):\n",
    "        '''searches for each category'''\n",
    "        cat_url = self.request_url(cat)\n",
    "        df = pd.read_html(cat_url)[0]\n",
    "        df['search_term']=cat\n",
    "        return df\n",
    "    \n",
    "    def get_bidurl(self,bidnum):\n",
    "        '''gets the bid url based on the bid registration number \n",
    "        (ones that do not have a proper bid registration number usually doesn't have a corresponding link and would ask the user to go to the organization website for more informatioin)'''\n",
    "        num_split = str(bidnum).split(sep='-')\n",
    "        bidno = num_split[0]\n",
    "        if len(bidno) == 11:\n",
    "            bidseq = num_split[-1]\n",
    "            bidurl = \"http://www.g2b.go.kr:8081/ep/invitation/publish/bidInfoDtl.do?bidno=\"+bidno+\"&bidseq=\"+bidseq\n",
    "            return bidurl\n",
    "        else: \n",
    "            return \"Check organization website (공고기관) for details\"\n",
    "        bidseq = refnum_split[-1]\n",
    "        bidurl = \"http://www.g2b.go.kr:8081/ep/invitation/publish/bidInfoDtl.do?bidno=\"+bidno+\"&bidseq=\"+bidseq\n",
    "        return bidurl\n",
    "\n",
    "    def scrape_categories(self, categories):\n",
    "        '''scrapes each keyword and compiles it into a list. \n",
    "        There is a 1 second delay between each search term to prevent getting blocked out of the site'''\n",
    "        appended_df = []\n",
    "        for category in tqdm(categories):\n",
    "            one_df = self.scrape_cat(category)\n",
    "            appended_df.append(one_df)\n",
    "            time.sleep(1)\n",
    "        appended_df = pd.concat(appended_df, axis = 0)\n",
    "        urlist=[]\n",
    "        for index,row in appended_df.iterrows():\n",
    "            urlist.append(self.get_bidurl(row['공고번호-차수']))\n",
    "            \n",
    "        appended_df['url']=urlist\n",
    "        return appended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to read txt files and parse the list\n",
    "def txt_reader(name):\n",
    "    with open(name+\".txt\",'rb') as f:\n",
    "        line = f.readline()\n",
    "        return line.decode('utf-8').split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:11<00:00,  3.13s/it]\n"
     ]
    }
   ],
   "source": [
    "#load the categories with the txt_reader function\n",
    "category_list = txt_reader('category')\n",
    "print(\"Getting the list of given keywords: \" +str(category_list).replace('[','').replace(']','').replace(\"'\",\"\"))\n",
    "\n",
    "#scrape with the \"KoreaPageScraper\" class\n",
    "myscraper = KoreaPageScraper()\n",
    "\n",
    "df = myscraper.scrape_categories(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440 results have been found. \n"
     ]
    }
   ],
   "source": [
    "print(str(len(df))+\" results have been found. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the excluding keywords\n",
    "with open('exclude.txt','rb') as f:\n",
    "    line = f.readline()\n",
    "    contains_excluding = line.decode('utf-8').replace('/','|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding the list of given keywords: 건설, 신축공사, 소모품 구매, 보강공사, 개보수공사, 연구\n"
     ]
    }
   ],
   "source": [
    "print(\"Excluding the list of given keywords: \"+str(txt_reader('exclude')).replace('[','').replace(']','').replace(\"'\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 entries with keywords to exclude. (Currently at 2440 entries)\n"
     ]
    }
   ],
   "source": [
    "#Deleting the excluding keywords and informing how many lines were deleted. \n",
    "og = len(df)\n",
    "df = df[-df.공고명.str.contains(contains_excluding).fillna(True)]\n",
    "print(\"Deleted \"+str(og-len(df))+\" entries with keywords to exclude. (Currently at \"+str(len(df))+\" entries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_up(df):\n",
    "    #Delete duplicates (more than two keywords together)\n",
    "    og2 = len(df)\n",
    "    df = df[~df.duplicated(['공고명'])].copy()\n",
    "    print(str(og2-len(df))+\" duplicates were found and deleted (Currently at \"+str(len(df))+\" entries)\")\n",
    "    #Divide the register date and due date\n",
    "    df['register_date'],df['duedate'] = df['입력일시(입찰마감일시)'].str.split('(', 1).str\n",
    "    df['duedate']=df['duedate'].str.replace(')','').replace('-','')\n",
    "    df = df.drop('입력일시(입찰마감일시)',axis=1)\n",
    "    #Sort the values by duedate. To sort with a different value, change the following line's 'duedate' with the column name you desire to sort it by. \n",
    "    column_sort = 'duedate'\n",
    "    df = df.sort_values(by=column_sort,ascending=False)\n",
    "    print(\"Values are sorted by the column '\"+column_sort+\"'. To change this, please talk to the tool owner. \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_prioritize(df,filter_list,column):\n",
    "    new_df = df[df[column].isin(filter_list)].copy()\n",
    "    new_df[str(column+\"_sorted\")] = pd.Categorical(new_df[column],categories=filter_list,ordered=True)\n",
    "    new_df = new_df.sort_values(column+\"_sorted\")\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 duplicates were found and deleted (Currently at 2398 entries)\n",
      "Values are sorted by the column 'duedate'. To change this, please talk to the tool owner. \n"
     ]
    }
   ],
   "source": [
    "#Cleaning up the df to make more sense\n",
    "clean_df = clean_up(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the entries from target organization list: 한국도로공사, 조달청, 방위사업청, 보건복지부, 대한체육회, 문화재청, 한국뇌연구원\n"
     ]
    }
   ],
   "source": [
    "#Get the target organization list\n",
    "org_list = txt_reader('orgs')\n",
    "print(\"Getting the entries from target organization list: \"+str(org_list).replace('[','').replace(']','').replace(\"'\",\"\"))\n",
    "org_df = filter_prioritize(clean_df,org_list,'공고기관')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class create_excel(object):\n",
    "    def get_length(self,column):\n",
    "        ##\n",
    "        ##This line is the problem!!\n",
    "        ##\n",
    "        valueex = column[~column.isnull()].reset_index(drop=True)[0]\n",
    "        if type(valueex) == str:\n",
    "            if valueex.startswith('=HYPERLINK'):\n",
    "                return len('Click link')\n",
    "            else: \n",
    "                len_list = list(column.dropna().apply(lambda x: len(str(x))))\n",
    "                maxlen = max(len_list)\n",
    "                medlen = np.median(len_list)\n",
    "                meanlen = np.mean(len_list)\n",
    "                diff = maxlen-medlen\n",
    "                stdlen = np.std(len_list)\n",
    "                #min(A,B+C*numchars)\n",
    "                if maxlen < 10:\n",
    "                    return maxlen+5\n",
    "                elif diff > 50:\n",
    "                    if medlen == 0:\n",
    "                        return min(55,meanlen+5)\n",
    "                    return medlen\n",
    "                elif maxlen < 50:\n",
    "                    return meanlen+15\n",
    "                else:\n",
    "                    return 50\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    def to_excel(self,df,name):\n",
    "        #Next step, format the excel file\n",
    "        print(\"saving the \"+name+\" list...\")\n",
    "        docname = \"나라장터_입찰공고-\"+name+\"-\"+str(strftime(\"%y%m%d(%H%M%S)\", localtime()))+\".xlsx\"\n",
    "        #make the destination directory, but guard against race condition\n",
    "        if not os.path.exists(name):\n",
    "            try:\n",
    "                os.makedirs(name)\n",
    "            except OSError as exc: \n",
    "                print(exc)\n",
    "                raise Exception('something failed')\n",
    "        writer = pd.ExcelWriter(\"%s/%s\"%(name,docname), engine='xlsxwriter')\n",
    "        df.to_excel(writer,index=False,sheet_name='Sheet1')\n",
    "        workbook  = writer.book\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        tablerange = xl_range(0,0,len(df),len(df.columns)-1)\n",
    "        headerrange = xl_range(0,0,0,len(df.columns)-1)\n",
    "        contentrange = xl_range(1,0,len(df),len(df.columns)-1)\n",
    "\n",
    "        #Formatting headers\n",
    "        header_format = workbook.add_format({'bg_color':'black'})\n",
    "        column_format = workbook.add_format({'bottom':True,'bg_color':'white'})\n",
    "        link_format = workbook.add_format({'font_color':'#157993','underline':True})\n",
    "        \n",
    "        # Set the column width and format.\n",
    "        columns = []\n",
    "        widths = []\n",
    "        for i in range(0,len(df.columns)):\n",
    "            a = xl_col_to_name(i)+\":\"+xl_col_to_name(i)\n",
    "            columns.append(a)\n",
    "            widths.append(self.get_length(df[df.columns[i]])) \n",
    "        \n",
    "        for c,w in zip(columns,widths):\n",
    "            worksheet.set_column(c, w)\n",
    "        \n",
    "        worksheet.conditional_format(contentrange,{'type':'no_errors',\n",
    "                                                   'format':column_format})\n",
    "        worksheet.conditional_format(headerrange,{'type':'no_errors',\n",
    "                                                  'format':header_format})\n",
    "        worksheet.conditional_format(tablerange,{'type':'text',\n",
    "                                                 'criteria':'containing',\n",
    "                                                 'value':'Click link',\n",
    "                                                 'format':link_format})\n",
    "           \n",
    "        #Formatting for putting in the header titles\n",
    "        table_headers = [{'header':c} for c in  df.columns]\n",
    "        #Create a table with the data\n",
    "        worksheet.add_table(tablerange,{'columns' : table_headers})         \n",
    "        \n",
    "        writer.save()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "go_to_excel = create_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the full list...\n"
     ]
    }
   ],
   "source": [
    "go_to_excel.to_excel(clean_df,'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the orgs list...\n"
     ]
    }
   ],
   "source": [
    "go_to_excel.to_excel(org_df,'orgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! Please hit Enter to exit this command prompt. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('All done! Please hit Enter to exit this command prompt. ')\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
